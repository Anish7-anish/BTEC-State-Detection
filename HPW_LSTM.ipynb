{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anish7-anish/BTEC-State-Detection/blob/main/HPW_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiV2ttUJ8UHy",
        "outputId": "49019fd9-314c-4c67-c923-48674e2a3224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: schedule in /usr/local/lib/python3.10/dist-packages (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp87CZgcVIfP"
      },
      "source": [
        "**Essential Libraries and Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFs9qwZXjOh5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, InputLayer\n",
        "import json\n",
        "from dateutil import tz\n",
        "from datetime import timedelta\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from dateutil import parser\n",
        "import io\n",
        "from email.mime.image import MIMEImage\n",
        "import schedule\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxFgCXQGTut7"
      },
      "source": [
        "**Secure Token Retrieval for GraphQL Authentication Using JWTold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqvIkyXK_xVx"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "def get_bearer_token(auth, password, name, role, url):\n",
        "    # Form and send the GraphQL request for authentication\n",
        "    auth_query = f\"\"\"\n",
        "        mutation authRequest {{\n",
        "            authenticationRequest(\n",
        "                input: {{authenticator: \"{auth}\", role: \"{role}\", userName: \"{name}\"}}\n",
        "            ) {{\n",
        "                jwtRequest {{\n",
        "                    challenge, message\n",
        "                }}\n",
        "            }}\n",
        "        }}\n",
        "    \"\"\"\n",
        "    auth_response = requests.post(url, json={'query': auth_query})\n",
        "    auth_response.raise_for_status()\n",
        "    jwt_request = auth_response.json()['data']['authenticationRequest']['jwtRequest']\n",
        "    if jwt_request['challenge'] is None:\n",
        "        raise requests.exceptions.HTTPError(jwt_request['message'])\n",
        "\n",
        "    # Form and send the GraphQL request for the token validation\n",
        "    validation_query = f\"\"\"\n",
        "        mutation authValidation {{\n",
        "            authenticationValidation(\n",
        "                input: {{authenticator: \"{auth}\", signedChallenge: \"{jwt_request['challenge']}|{password}\"}}\n",
        "            ) {{\n",
        "                jwtClaim\n",
        "            }}\n",
        "        }}\n",
        "    \"\"\"\n",
        "    validation_response = requests.post(url, json={'query': validation_query})\n",
        "    validation_response.raise_for_status()\n",
        "    jwt_claim = validation_response.json()['data']['authenticationValidation']['jwtClaim']\n",
        "    return f\"Bearer {jwt_claim}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBBvc5UVTZCC"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElQvATS1Tpuh"
      },
      "source": [
        "**Enhanced GraphQL Request Handling with Token Management and Error Control**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdtIsbTC__ee"
      },
      "outputs": [],
      "source": [
        "def perform_graphql_request(query, url, auth, password, name, role, current_token=None):\n",
        "    # If no token was provided or if the token is expired, get a new one\n",
        "    if current_token is None:\n",
        "        current_token = get_bearer_token(auth, password, name, role, url)\n",
        "    headers = {\"Authorization\": current_token}\n",
        "\n",
        "    # Perform the request with the token\n",
        "    response = requests.post(url, json={'query': query}, headers=headers)\n",
        "    if response.status_code == 401:  # Unauthorized, token has expired\n",
        "        current_token = get_bearer_token(auth, password, name, role, url)\n",
        "        headers = {\"Authorization\": current_token}\n",
        "        response = requests.post(url, json={'query': query}, headers=headers)\n",
        "\n",
        "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGNZTDtTTIlu"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzE8_1tw6tq6"
      },
      "outputs": [],
      "source": [
        "def preprocess_realtime_data(data, sequence_length):\n",
        "    # Convert string values to 'Off' and 'On' for RO_Skid_Running column\n",
        "    for ts, entry in data.items():\n",
        "        if entry[\"RO_Skid_Running\"] == \"None\":\n",
        "            entry[\"RO_Skid_Running\"] = np.nan\n",
        "        elif entry[\"RO_Skid_Running\"] == \"0\":\n",
        "            entry[\"RO_Skid_Running\"] = \"Off\"\n",
        "        elif entry[\"RO_Skid_Running\"] == \"1\":\n",
        "            entry[\"RO_Skid_Running\"] = \"On\"\n",
        "\n",
        "    # Convert 'None' or NaN values to np.nan for Level_PV column\n",
        "    for ts, entry in data.items():\n",
        "        if entry[\"Level_PV\"] is None or np.isnan(entry.get(\"Level_PV\", np.nan)):\n",
        "            entry[\"Level_PV\"] = np.nan\n",
        "\n",
        "    # Convert data to DataFrame for better visualization\n",
        "    df = pd.DataFrame.from_dict(data, orient='index')\n",
        "\n",
        "    # Forward fill the missing values for \"Level_PV\" and \"RO_Skid_Running\"\n",
        "    df['Level_PV'] = df['Level_PV'].fillna(method='ffill')\n",
        "    df['RO_Skid_Running'] = df['RO_Skid_Running'].fillna(method='ffill')\n",
        "\n",
        "    # After forward fill, if there are still NaNs at the start, fill with the first non-NaN value\n",
        "    df['Level_PV'].fillna(method='bfill', inplace=True)\n",
        "    df['RO_Skid_Running'].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    # Create sequences of the same length as model input sequence length\n",
        "    X_realtime = []\n",
        "    timestamps = list(df.index)  # Keep track of timestamps to maintain the integrity of sequences\n",
        "\n",
        "    for i in range(len(df) - sequence_length + 1):\n",
        "        sequence = [df['Level_PV'].iloc[j] for j in range(i, i + sequence_length)]\n",
        "        X_realtime.append(sequence)\n",
        "\n",
        "    return np.array(X_realtime), df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omf4mkx-S1vv"
      },
      "source": [
        "**Json File Generation and Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otfUyImy6u6C"
      },
      "outputs": [],
      "source": [
        "def save_state_info(state_info, file_path='state_info.json'):\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(state_info, file)\n",
        "\n",
        "def load_state_info(file_path='state_info.json'):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        return {\"current_state\": \"normal\", \"last_state_change_time\": datetime.now(pytz.utc).strftime('%Y-%m-%d %H:%M:%S+00:00')}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbpfAkOe0H0i"
      },
      "outputs": [],
      "source": [
        "def modified_z_scores(observations):\n",
        "    median_y = np.median(observations)\n",
        "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in observations])\n",
        "    modified_z_scores = [0.6745 * (y - median_y) / median_absolute_deviation_y if median_absolute_deviation_y else 0 for y in observations]\n",
        "    return modified_z_scores\n",
        "\n",
        "def dynamic_thresholding(data, window_size=50, threshold_factor=3.5):\n",
        "    alerts = []\n",
        "    for i in range(len(data)):\n",
        "        if i < window_size:\n",
        "            window_data = data[:i+1]\n",
        "        else:\n",
        "            window_data = data[i-window_size:i]\n",
        "\n",
        "        scores = modified_z_scores(window_data)\n",
        "        if np.abs(scores[-1]) > threshold_factor:\n",
        "            alerts.append(i)  # index of the anomaly\n",
        "    return alerts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrQGtr52Srh-"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j-7HyeG7Qxx"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "    processed_data = {}\n",
        "    prev_stringvalue = None\n",
        "    for entry in data:\n",
        "        ts_str = entry[\"ts\"]\n",
        "        ts = parser.isoparse(ts_str)\n",
        "        level_pv = entry[\"floatvalue\"]  # Rename \"floatvalue\" to \"Level_PV\"\n",
        "        ro_skid_running = entry[\"stringvalue\"]  # Rename \"stringvalue\" to \"RO_Skid_Running\"\n",
        "        if ro_skid_running is None:\n",
        "            last_ts = max(processed_data.keys()) if processed_data else None\n",
        "            if last_ts and (ts - last_ts) <= timedelta(minutes=10):\n",
        "                ro_skid_running = prev_stringvalue\n",
        "        else:\n",
        "            prev_stringvalue = ro_skid_running\n",
        "        processed_data[ts] = {'Level_PV': level_pv, 'RO_Skid_Running': ro_skid_running}\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWp7-DOy7Qoh"
      },
      "outputs": [],
      "source": [
        "def filter_anomalies(anomalies, window=3):\n",
        "    # Create an array to hold the filtered anomalies\n",
        "    filtered_anomalies = []\n",
        "    for index in anomalies:\n",
        "        # Check if the next `window` points are also anomalies\n",
        "        if all(i in anomalies for i in range(index, index + window)):\n",
        "            filtered_anomalies.append(index)\n",
        "\n",
        "    # Return the unique values to avoid duplicate indices\n",
        "    return np.unique(filtered_anomalies)\n",
        "\n",
        "def adaptive_percentile_threshold(errors, window_size=100, percentile=95):\n",
        "    # Calculate the rolling percentile\n",
        "    if len(errors) < window_size:\n",
        "        # Not enough data to fill the window, use available data\n",
        "        return np.percentile(errors, percentile)\n",
        "    else:\n",
        "        # Use the last `window_size` errors to calculate the percentile\n",
        "        return np.percentile(errors[-window_size:], percentile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivqOW2-R7Qez"
      },
      "outputs": [],
      "source": [
        "def generate_query_timestamps(days=6):\n",
        "    \"\"\"\n",
        "    Generates query timestamps for the past 'hours' hours until now.\n",
        "\n",
        "    :param hours: The number of hours to look back from the current time.\n",
        "    :return: A tuple containing the start and end timestamps as strings.\n",
        "    \"\"\"\n",
        "    # Assuming UTC time zone. Change 'utc' to another time zone if needed\n",
        "    utc_zone = pytz.utc\n",
        "    end_time = datetime.now(utc_zone)\n",
        "    start_time = end_time - timedelta(days=days)\n",
        "\n",
        "    # Format timestamps as required for your GraphQL query\n",
        "    time_format = \"%Y-%m-%d %H:%M:%S+00\"\n",
        "    return start_time.strftime(time_format), end_time.strftime(time_format)\n",
        "\n",
        "\n",
        "def generate_query_timestamps_training(days=6, training_days=22):\n",
        "    \"\"\"\n",
        "    Generates query timestamps for the past 'days' days until now, and adjusts the start time\n",
        "    to consider the training data period.\n",
        "\n",
        "    :param days: The total number of days to look back from the current time.\n",
        "    :param training_days: The number of days to use for training data.\n",
        "    :return: A tuple containing the start and end timestamps as strings.\n",
        "    \"\"\"\n",
        "    # Assuming UTC time zone. Change 'utc' to another time zone if needed\n",
        "    utc_zone = pytz.utc\n",
        "    end_time = datetime.now(utc_zone) - timedelta(days=days)\n",
        "    start_time = end_time - timedelta(days=training_days)\n",
        "\n",
        "    # Adjust start time to consider training data period\n",
        "    time_format = \"%Y-%m-%d %H:%M:%S+00\"\n",
        "    return start_time.strftime(time_format), end_time.strftime(time_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgwYGbUZ7QT9"
      },
      "outputs": [],
      "source": [
        "def send_email(subject, message, original_plot_buffer=None, zoomed_plot_buffer=None):\n",
        "\n",
        "    sender_email = 'newvalo49@gmail.com'\n",
        "    receiver_email = 'newvalo49@gmail.com'\n",
        "    password = 'kntu yxlv bzrg sxmp'\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = sender_email\n",
        "    msg['To'] = receiver_email\n",
        "    msg['Subject'] = subject\n",
        "    msg.attach(MIMEText(message, 'plain'))\n",
        "\n",
        "    if original_plot_buffer:\n",
        "        original_img = MIMEImage(original_plot_buffer.getvalue())\n",
        "        original_img.add_header('Content-Disposition', 'attachment', filename=\"original_state_change_plot.png\")\n",
        "        msg.attach(original_img)\n",
        "\n",
        "    if zoomed_plot_buffer:\n",
        "        zoomed_img = MIMEImage(zoomed_plot_buffer.getvalue())\n",
        "        zoomed_img.add_header('Content-Disposition', 'attachment', filename=\"zoomed_state_change_plot.png\")\n",
        "        msg.attach(zoomed_img)\n",
        "\n",
        "    try:\n",
        "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "        server.starttls()\n",
        "        server.login(sender_email, password)\n",
        "        server.sendmail(sender_email, receiver_email, msg.as_string())\n",
        "        print('Email sent successfully!')\n",
        "    except Exception as e:\n",
        "        print(f\"Error sending email: {e}\")\n",
        "    finally:\n",
        "        server.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-hEJnPRMQ0T"
      },
      "outputs": [],
      "source": [
        "def dynamic_thresholding(errors, window_size=100, percentile=95):\n",
        "    if len(errors) < window_size:\n",
        "        threshold = np.percentile(errors, percentile)  # Fallback to a default threshold\n",
        "    else:\n",
        "        threshold = np.percentile(errors[-window_size:], percentile)\n",
        "\n",
        "    anomalies = [i for i, error in enumerate(errors) if error > threshold]\n",
        "    return anomalies, threshold\n",
        "\n",
        "def plot_data_with_threshold(values, anomalies, threshold):\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(values, label='Data')\n",
        "    plt.axhline(y=threshold, color='r', linestyle='-', label='Dynamic Threshold')\n",
        "    plt.scatter(anomalies, [values[i] for i in anomalies], color='magenta', label='Anomalies')\n",
        "    plt.legend()\n",
        "    plt.title('Data and Anomalies with Dynamic Threshold')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Values')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoS9r84WSY6z"
      },
      "source": [
        "**The Plotting logic**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI-KtLRsDgfN"
      },
      "outputs": [],
      "source": [
        "# Ensure pandas is imported for datetime operations\n",
        "def create_plot(pre_df_filtered, state_changes, recent_state_changes, filtered_anomalies_realtime_adjusted):\n",
        "    plt.figure(figsize=(20, 8))\n",
        "    plt.plot(pre_df_filtered.index, pre_df_filtered['Level_PV'], label='Tank Level', color='blue')\n",
        "\n",
        "    # Plot vertical lines for all state changes\n",
        "    for change_time, state in state_changes:\n",
        "        color = 'red' if state == \"high\" else 'green'\n",
        "        plt.axvline(x=change_time, color=color, linestyle='--', label=f'State changed to {state}')\n",
        "\n",
        "    # Highlight recent state changes in grey\n",
        "    for change_time, state in recent_state_changes:\n",
        "        plt.axvline(x=change_time, color='grey', linestyle=':', linewidth=6, label='Recent State Change')\n",
        "\n",
        "    for anomaly in filtered_anomalies_realtime_adjusted:  # Modified line\n",
        "        plt.axvline(x=pre_df_filtered.index[anomaly], color='purple', linestyle='--', linewidth=2, label='Anomaly')  # Added line\n",
        "\n",
        "    # Ensure each legend entry is unique\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    unique_labels_handles = dict(zip(labels, handles)).items()\n",
        "    sorted_handles_labels = sorted(unique_labels_handles)\n",
        "    plt.legend([handle for label, handle in sorted_handles_labels], [label for label, handle in sorted_handles_labels])\n",
        "\n",
        "    plt.title('Tank Level with State Changes')\n",
        "    plt.xlabel('Timestamp')\n",
        "    plt.ylabel('Tank Level')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert plot to a BytesIO object for the original data span\n",
        "    original_plot_buffer = io.BytesIO()\n",
        "    plt.savefig(original_plot_buffer, format='png')\n",
        "    plt.close()\n",
        "    original_plot_buffer.seek(0)\n",
        "\n",
        "    # Create a second plot focusing on the last 3 hours of data\n",
        "    plt.figure(figsize=(20, 8))\n",
        "    plt.plot(pre_df_filtered.index, pre_df_filtered['Level_PV'], label='Tank Level', color='blue')\n",
        "\n",
        "    # Plot vertical lines for all state changes\n",
        "    for change_time, state in state_changes:\n",
        "        color = 'red' if state == \"high\" else 'green'\n",
        "        plt.axvline(x=change_time, color=color, linestyle='--', label=f'State changed to {state}')\n",
        "\n",
        "    # Highlight recent state changes in grey\n",
        "    for change_time, state in recent_state_changes:\n",
        "        plt.axvline(x=change_time, color='grey', linestyle=':', linewidth=6, label='Recent State Change')\n",
        "\n",
        "    for anomaly in filtered_anomalies_realtime_adjusted:  # Modified line\n",
        "        plt.axvline(x=pre_df_filtered.index[anomaly], color='purple', linestyle='--', linewidth=2, label='Anomaly')\n",
        "\n",
        "    # Ensure each legend entry is unique\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    unique_labels_handles = dict(zip(labels, handles)).items()\n",
        "    sorted_handles_labels = sorted(unique_labels_handles)\n",
        "    plt.legend([handle for label, handle in sorted_handles_labels], [label for label, handle in sorted_handles_labels])\n",
        "\n",
        "    plt.title('Tank Level with State Changes (Last 12 Hours)')\n",
        "    plt.xlabel('Timestamp')\n",
        "    plt.ylabel('Tank Level')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Zoom in on the last 3 hours of data\n",
        "    last_time = pre_df_filtered.index[-1]\n",
        "    plt.xlim(last_time - pd.Timedelta(hours=12), last_time)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert plot to a BytesIO object for the zoomed view\n",
        "    zoomed_plot_buffer = io.BytesIO()\n",
        "    plt.savefig(zoomed_plot_buffer, format='png')\n",
        "    plt.close()\n",
        "    zoomed_plot_buffer.seek(0)\n",
        "\n",
        "    return original_plot_buffer, zoomed_plot_buffer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKROmzgiSDPR"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDDsQZplDm2e"
      },
      "outputs": [],
      "source": [
        "def train_and_save_model():\n",
        "    start_time, end_time = generate_query_timestamps_training(days=6, training_days=22)\n",
        "    print(start_time)\n",
        "    print(end_time)  # Train with data from the last 30 days\n",
        "    print(\"Training started NOW\")\n",
        "\n",
        "    authenticator = \"Anish\"\n",
        "    password = \"Msdhoni#7ncsu\"\n",
        "    name = \"atoorpu\"\n",
        "    role = \"ncsu_group\"\n",
        "    instance_graphql_endpoint = \"https://ncsu.cesmii.net/graphql\"\n",
        "\n",
        "    graphql_query = f\"\"\"\n",
        "    query HistoryQuery {{\n",
        "        getRawHistoryDataWithSampling(\n",
        "            maxSamples: 0\n",
        "            ids: [\"764692\",\"764782\"]\n",
        "            startTime: \"{start_time}\"\n",
        "            endTime: \"{end_time}\"\n",
        "        ) {{\n",
        "            ts\n",
        "            floatvalue\n",
        "            stringvalue\n",
        "        }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response_data = perform_graphql_request(graphql_query, instance_graphql_endpoint, authenticator, password, name, role)\n",
        "\n",
        "    # Extract data from response\n",
        "    data = response_data[\"data\"][\"getRawHistoryDataWithSampling\"]\n",
        "\n",
        "    # Preprocess the data\n",
        "    processed_data = preprocess_data(data)\n",
        "    X_realtime, pre_df = preprocess_realtime_data(processed_data, sequence_length=5)\n",
        "\n",
        "    print(\"X_realtime\",X_realtime)\n",
        "\n",
        "    # Creating the sequences needed for LSTM input\n",
        "    X = [pre_df['Level_PV'].values[i:i + 5] for i in range(len(pre_df) - 5)]\n",
        "    X = np.array(X)\n",
        "    switch_states = pre_df['RO_Skid_Running'].values[4:]  # Adjust the index as per your sequence length\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, switch_states_train, switch_states_test = train_test_split(\n",
        "        X, switch_states[:-1], test_size=0.2, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Define and train the model\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(X_train.shape[1], 1)),\n",
        "        LSTM(50, activation='relu'),\n",
        "        RepeatVector(X_train.shape[1]),\n",
        "        LSTM(50, activation='relu', return_sequences=True),\n",
        "        TimeDistributed(Dense(1))\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
        "    model.fit(X_train, X_train, epochs=100, batch_size=32, validation_data=(X_test, X_test))\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save('lstm_model.h5')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s72qsb1TRzsZ"
      },
      "source": [
        "**Anomaly Detection and State Monitoring with LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6qyru09HTaw"
      },
      "outputs": [],
      "source": [
        "def real_time_monitoring():\n",
        "    start_time, end_time = generate_query_timestamps(6)\n",
        "    model_path = 'lstm_model.h5'\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Model file not found. Please train the model first.\")\n",
        "        return  # Exit the function if model is not found\n",
        "\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    print(\"Model loaded successfully for monitoring.\")\n",
        "\n",
        "    print(\"hiiiii\")\n",
        "    print(start_time)\n",
        "    print(end_time)\n",
        "    state_info = load_state_info()\n",
        "    current_state = state_info['current_state']\n",
        "    print(\"Current State\", current_state)\n",
        "    utc_zone = tz.tzutc()\n",
        "    last_state_change_time = datetime.strptime(state_info['last_state_change_time'], '%Y-%m-%d %H:%M:%S+00:00')\n",
        "    print(\"last_stage_change_time\",last_state_change_time)\n",
        "    now = datetime.now(pytz.utc)\n",
        "    should_save_state_info = False\n",
        "\n",
        "    authenticator = \"Anish\"\n",
        "    password = \"Msdhoni#7ncsu\"\n",
        "    name = \"atoorpu\"\n",
        "    role = \"ncsu_group\"\n",
        "    instance_graphql_endpoint = \"https://ncsu.cesmii.net/graphql\"\n",
        "\n",
        "\n",
        "    graphql_query = f\"\"\"\n",
        "    query HistoryQuery {{\n",
        "        getRawHistoryDataWithSampling(\n",
        "            maxSamples: 0\n",
        "            ids: [\"764692\",\"764782\"]\n",
        "            startTime: \"{start_time}\"\n",
        "            endTime: \"{end_time}\"\n",
        "        ) {{\n",
        "            ts\n",
        "            floatvalue\n",
        "            stringvalue\n",
        "        }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # Here you would fetch and preprocess your data to get X_realtime\n",
        "    # For example, let's assume you've done this and have your data ready in X_realtime\n",
        "    response_data = perform_graphql_request(graphql_query, instance_graphql_endpoint, authenticator, password, name, role)\n",
        "    print(response_data)\n",
        "\n",
        "\n",
        "    # Extract data from response\n",
        "    data = response_data[\"data\"][\"getRawHistoryDataWithSampling\"]\n",
        "    print(data)\n",
        "\n",
        "    stringvalue_dict = preprocess_data(data)\n",
        "\n",
        "    sequence_length = 5\n",
        "    X_realtime,pre_df = preprocess_realtime_data(stringvalue_dict, sequence_length)\n",
        "\n",
        "    print(\"pre_df\",pre_df)\n",
        "    print(len(pre_df))\n",
        "\n",
        "    predictions = model.predict(X_realtime)\n",
        "    predictions = predictions.squeeze(axis=-1)\n",
        "    print(\"predictions\",predictions)\n",
        "    if pre_df.index.tzinfo is None or pre_df.index.tzinfo.utcoffset(pre_df.index[0]) is None:\n",
        "        pre_df.index = pd.to_datetime(pre_df.index).tz_localize(tz.tzutc())\n",
        "    else:\n",
        "        pre_df.index = pre_df.index.tz_convert(tz.tzutc())\n",
        "\n",
        "    # start_time=\"2024-04-06 12:32:33.513000+00:00\"\n",
        "    # end_time=\"2024-04-11 17:00:17.263000+00:00\"\n",
        "    pre_df_filtered = pre_df[(pre_df.index >= pd.to_datetime(start_time)) & (pre_df.index <= pd.to_datetime(end_time))]\n",
        "    print(\"pre_df_filtered\",pre_df_filtered)\n",
        "    print(len(pre_df_filtered))\n",
        "\n",
        "\n",
        "    # Include your anomaly detection and state change logic here\n",
        "    switch_states_realtime = np.array([entry['RO_Skid_Running'] for entry in stringvalue_dict.values()][sequence_length - 1:])\n",
        "    print(switch_states_realtime)\n",
        "\n",
        "    reconstruction_errors_realtime = np.mean(np.square(X_realtime - predictions), axis=1)\n",
        "    print(reconstruction_errors_realtime)\n",
        "\n",
        "    level_pv_last_values = [sequence[-1] for sequence in X_realtime]\n",
        "\n",
        "    off_state_errors_adjusted = reconstruction_errors_realtime[(switch_states_realtime == 'Off') & (np.array(level_pv_last_values) < 85)]\n",
        "\n",
        "    threshold_realtime_adjusted = np.percentile(off_state_errors_adjusted, 97)\n",
        "    #threshold_realtime_adjusted = adaptive_percentile_threshold(off_state_errors_adjusted, window_size=100, percentile=95)\n",
        "    print(\"threshold_realtime_adjusted\",threshold_realtime_adjusted)\n",
        "\n",
        "    anomalies_realtime_adjusted = np.where((reconstruction_errors_realtime > threshold_realtime_adjusted) & (switch_states_realtime == 'Off') & (np.array(level_pv_last_values) < 85))[0]\n",
        "    print(anomalies_realtime_adjusted)\n",
        "\n",
        "    filtered_anomalies_realtime_adjusted = filter_anomalies(anomalies_realtime_adjusted)\n",
        "    last_state_change_time = last_state_change_time.replace(tzinfo=tz.tzutc())\n",
        "    # Example update of state_info (this should be replaced with your actual logic)\n",
        "    anomaly_presence_threshold = 2 # Threshold for consecutive anomalies to consider a state change\n",
        "    persistence_requirement = timedelta(hours=12)  # Required duration for anomalies to persist\n",
        "\n",
        "    # Initialize variables for state monitoring\n",
        "    state_changes = []\n",
        "    current_anomaly_sequence = 0\n",
        "    print(current_state)\n",
        "\n",
        "    local_current_state = current_state  # Start with the global state\n",
        "    local_last_state_change_time = last_state_change_time  # Start with the global last change time\n",
        "\n",
        "    # Process anomalies and determine state changes\n",
        "    for i in range(len(pre_df_filtered)):\n",
        "        is_anomaly = i in filtered_anomalies_realtime_adjusted\n",
        "        time_since_last_change = pre_df_filtered.index[i] - local_last_state_change_time\n",
        "\n",
        "        if is_anomaly:\n",
        "            current_anomaly_sequence += 1\n",
        "        else:\n",
        "            current_anomaly_sequence = 0\n",
        "\n",
        "        state_changed = False\n",
        "        new_state = None\n",
        "        if current_anomaly_sequence >= anomaly_presence_threshold and local_current_state == \"normal\":\n",
        "            new_state = \"high\"\n",
        "            local_current_state = new_state  # Update local state\n",
        "            local_last_state_change_time = pre_df_filtered.index[i]  # Update local time of change\n",
        "            state_changes.append((local_last_state_change_time, local_current_state))\n",
        "\n",
        "        elif not is_anomaly and local_current_state == \"high\" and time_since_last_change >= persistence_requirement:\n",
        "            new_state = \"normal\"\n",
        "            local_current_state = new_state  # Update local state\n",
        "            local_last_state_change_time = pre_df_filtered.index[i]  # Update local time of change\n",
        "            state_changes.append((local_last_state_change_time, local_current_state))\n",
        "\n",
        "\n",
        "    print(\"current state\", current_state)\n",
        "    print(\"state_changes\",state_changes)\n",
        "    recent_state_changes=[]\n",
        "\n",
        "    # utc_now = datetime.now(pytz.utc)\n",
        "    # time = \"2024-04-11 17:00:17.263000+00:00\"\n",
        "    # utc_now = datetime.strptime(time, '%Y-%m-%d %H:%M:%S.%f%z')\n",
        "    utc_zone = pytz.utc\n",
        "    utc_now = datetime.now(utc_zone)\n",
        "    fifteen_minutes_ago = utc_now - timedelta(minutes=15)\n",
        "    print(\"fifteen minutes ago\",fifteen_minutes_ago)\n",
        "\n",
        "    # Filter state changes that occurred within the last 15 minutes\n",
        "    recent_state_changes = [change for change in state_changes if change[0] >= fifteen_minutes_ago]\n",
        "    print(recent_state_changes)\n",
        "\n",
        "    for change_time, new_state in recent_state_changes:\n",
        "      print(change_time)\n",
        "      print(new_state)\n",
        "\n",
        "\n",
        "    # Check if there are any recent changes and process them\n",
        "    should_save_state_info = False\n",
        "    for change_time, new_state in recent_state_changes:\n",
        "        print(\"new_state\",new_state)\n",
        "        print(\"current_state\",current_state)\n",
        "        print(new_state != current_state)\n",
        "        if new_state != current_state:\n",
        "            # Update the current state and the time of change\n",
        "            current_state = new_state\n",
        "            last_state_change_time = change_time\n",
        "            should_save_state_info = True\n",
        "\n",
        "\n",
        "            original_plot_buffer, zoomed_plot_buffer = create_plot(pre_df_filtered, state_changes, recent_state_changes, filtered_anomalies_realtime_adjusted)\n",
        "            original_plot_image = plt.imread(original_plot_buffer)\n",
        "            zoomed_plot_image = plt.imread(zoomed_plot_buffer)\n",
        "\n",
        "            # Prepare and send email about the state change\n",
        "            subject = f\"CESMII SMIP: HPW V8000 State Change Notification: {current_state}\"\n",
        "\n",
        "            est_timezone = pytz.timezone('America/New_York')\n",
        "\n",
        "            # Convert last_state_change_time to EST and format it\n",
        "            last_state_change_time_est = last_state_change_time.astimezone(est_timezone).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "            # Customizing the message for each state change\n",
        "            if new_state == \"high\":\n",
        "                message = (f\"The HPW system V8000 has entered a state of high water demand as of {last_state_change_time_est}. \"\n",
        "                   \"Please change V8000 process setpoints to the following:\\n\\n\"\n",
        "                   \"Return Pressure Setpoint: 44 psig\\n\"\n",
        "                   \"Supply Pressure Setpoint: 78 psig\")\n",
        "            else:\n",
        "                message = (f\"The HPW system V8000 has entered a normal state with low water usage as of {last_state_change_time_est}. \"\n",
        "                   \"Please change V8000 process setpoints to the following:\\n\\n\"\n",
        "                   \"Return Pressure Setpoint: 40 psig\\n\"\n",
        "                   \"Supply Pressure Setpoint: 64 psig\")\n",
        "            send_email(subject, message, original_plot_buffer, zoomed_plot_buffer)\n",
        "\n",
        "    if should_save_state_info:\n",
        "        # Save the state info if there's a recent change\n",
        "        updated_state_info = {\n",
        "            \"current_state\": current_state,\n",
        "            \"last_state_change_time\": last_state_change_time.strftime('%Y-%m-%d %H:%M:%S+00:00')\n",
        "        }\n",
        "        save_state_info(updated_state_info)\n",
        "    else:\n",
        "        # Log a message if no recent changes occurred\n",
        "        print(\"No recent changes. Retaining previous state.\")\n",
        "\n",
        "    print(\"Recent changes within the last 15 minutes:\", [f\"State changed to {s[1]} at {s[0].strftime('%Y-%m-%d %H:%M:%S+00:00')}\" for s in recent_state_changes])\n",
        "\n",
        "    plt.figure(figsize=(20, 8))\n",
        "    plt.plot(pre_df_filtered.index, pre_df_filtered['Level_PV'], label='Tank Level', color='blue')\n",
        "\n",
        "    # Plot vertical lines for all state changes\n",
        "    for change_time, state in state_changes:\n",
        "        if state == \"high\":\n",
        "            color = 'red'\n",
        "        else:\n",
        "            color = 'green'\n",
        "        plt.axvline(x=change_time, color=color, linestyle='--', label=f'State changed to {state}')\n",
        "\n",
        "    # Highlight recent state changes in purple\n",
        "    for change_time, state in recent_state_changes:\n",
        "        plt.axvline(x=change_time, color='purple', linestyle='--', linewidth=6, label='Recent State Change')\n",
        "\n",
        "    # This part ensures that each state change label is only added to the legend once\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    unique_labels_handles = dict(zip(labels, handles)).items()\n",
        "    sorted_handles_labels = sorted(unique_labels_handles)  # Sort or customize as needed\n",
        "    plt.legend([handle for label, handle in sorted_handles_labels], [label for label, handle in sorted_handles_labels])\n",
        "\n",
        "    plt.title('Tank Level with State Changes')\n",
        "    plt.xlabel('Timestamp')\n",
        "    plt.ylabel('Tank Level')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Display the number of state changes\n",
        "    print(f\"Total state changes: {len(state_changes)}\")\n",
        "    for change_time, state in state_changes:\n",
        "        print(f\"State changed to {state} at {change_time}\")\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Plot the tank level\n",
        "    plt.plot(pre_df_filtered.index, pre_df_filtered['Level_PV'], label='Tank Level', color='blue')\n",
        "\n",
        "    # Plot the anomalies using plt.plot\n",
        "    for anomaly in filtered_anomalies_realtime_adjusted:\n",
        "        anomaly_start_index = anomaly\n",
        "        anomaly_end_index = anomaly + sequence_length\n",
        "        # Plot the segment of the anomaly\n",
        "        plt.plot(pre_df_filtered.index[anomaly_start_index:anomaly_end_index],\n",
        "                pre_df_filtered['Level_PV'][anomaly_start_index:anomaly_end_index],\n",
        "                'r-', label='Anomaly' if 'Anomaly' not in [l.get_label() for l in plt.gca().get_lines()] else \"\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title('Real-time Tank Level with Anomalies (LSTM)')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Tank Level')\n",
        "    plt.show()\n",
        "\n",
        "# Schedule the real_time_monitoring function to run every minute\n",
        "# schedule.every(15).minutes.do(real_time_monitoring)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists('lstm_model.h5'):\n",
        "        print(\"No model found. Training now...\")\n",
        "        train_and_save_model()\n",
        "\n",
        "    # Initial monitoring to start the process\n",
        "    real_time_monitoring()\n",
        "\n",
        "    # Set up scheduled tasks\n",
        "    # schedule.every(30).days.do(train_and_save_model)\n",
        "    schedule.every(30).minutes.do(train_and_save_model)\n",
        "    schedule.every(15).minutes.do(real_time_monitoring)\n",
        "\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI9DA57dRbGt"
      },
      "source": [
        "**This is written for sample logging in cesmii platorm using thinkiq.context.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFHjTiD80yi1"
      },
      "outputs": [],
      "source": [
        "from thinkiq_context import get_context\n",
        "context = get_context()                                 # the context object holds runtime information such as the equipment id and data stored between runs\n",
        "equipment_id = context.std_inputs.node_id               # the id of the equipment instance to use\n",
        "print(context.std_inputs.parent_id)\n",
        "\n",
        "from thinkiq.model.equipment import Equipment\n",
        "from thinkiq.model.node import Node\n",
        "eqpt = Node.get_from_id(equipment_id)              # creates an object for this equipment item\n",
        "context.logger.info(f'Running script {context.std_inputs.script_name} on {eqpt.display_name}')   # shows how to use the context object and equipment model\n",
        "context.return_data({  \"brand\": \"Ford\",  \"model\": \"Mustang\",  \"year\": 1964})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLQOWR0Yx8DT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyM/ZahTS8Ap+SqCIVZVP7rm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}